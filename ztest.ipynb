{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d28e7576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/slim-face-recognition/src/slim_face/models\n"
     ]
    }
   ],
   "source": [
    "%cd /workspaces/slim-face-recognition/src/slim_face/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5029ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading YOLOv11 face detection model...\n",
      "Git LFS is installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into './checkpoints/yolo11_face_detection'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cloned repository to ./checkpoints/yolo11_face_detection\n",
      "Updated Git hooks.\n",
      "Git LFS initialized.\n",
      "Git LFS initialized.\n",
      "\"*.pt\" already supported\n",
      "\"*.bin\" already supported\n",
      "Tracked large files with Git LFS.\n",
      "Successfully pulled large files with Git LFS.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoints/yolo11_face_detection/model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m batch_size = \u001b[38;5;28mlen\u001b[39m(paths) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(paths, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Align faces (assuming align.get_aligned_face returns a list of tuples)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m aligned_result = \u001b[43malign\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_aligned_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43myolo\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m transformed_inputs = [transform(result[\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m aligned_result]\n\u001b[32m     25\u001b[39m transformed_inputs = torch.stack(transformed_inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/slim-face-recognition/src/slim_face/models/edgeface/face_alignment/align.py:52\u001b[39m, in \u001b[36mget_aligned_face\u001b[39m\u001b[34m(image_path_or_image_paths, rgb_pil_image, algorithm)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_path_or_image_paths, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     51\u001b[39m     image_paths = [image_path_or_image_paths]\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     results = \u001b[43mface_yolo_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# yolo_model_path=\"checkpoints/yolo11_face_detection/model.pt\",\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m                \u001b[49m\u001b[43muse_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mimage_path_or_image_paths must be a list or string\u001b[39m\u001b[33m\"\u001b[39m) \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/slim-face-recognition/src/slim_face/models/edgeface/face_alignment/face_yolo.py:87\u001b[39m, in \u001b[36mface_yolo_detection\u001b[39m\u001b[34m(image_paths, yolo_model_path, use_batch)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mface_yolo_detection\u001b[39m(image_paths,\n\u001b[32m     83\u001b[39m                         yolo_model_path=\u001b[33m\"\u001b[39m\u001b[33m./checkpoints/yolo11_face_detection/model.pt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     84\u001b[39m                         use_batch=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# print(\"os.path.abspath(yolo_model_path) \", os.path.abspath(yolo_model_path))\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Perform face detection using YOLOv11 with batch or individual processing.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     model = \u001b[43minitialize_yolo_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     all_bounding_boxes, all_cropped_faces = [], []\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_batch:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/slim-face-recognition/src/slim_face/models/edgeface/face_alignment/face_yolo.py:18\u001b[39m, in \u001b[36minitialize_yolo_model\u001b[39m\u001b[34m(yolo_model_path)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(yolo_model_path):\n\u001b[32m     17\u001b[39m     download_yolo_face_detection.download_yolo_face_detection_model()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo_model_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ultralytics/models/yolo/model.py:79\u001b[39m, in \u001b[36mYOLO.__init__\u001b[39m\u001b[34m(self, model, task, verbose)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m = new_instance.\u001b[34m__dict__\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mRTDETR\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.model[-\u001b[32m1\u001b[39m]._get_name():  \u001b[38;5;66;03m# if RTDETR head\u001b[39;00m\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RTDETR\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ultralytics/engine/model.py:151\u001b[39m, in \u001b[36mModel.__init__\u001b[39m\u001b[34m(self, model, task, verbose)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28mself\u001b[39m._new(model, task=task, verbose=verbose)\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# Delete super().training for accessing self.model.training\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ultralytics/engine/model.py:295\u001b[39m, in \u001b[36mModel._load\u001b[39m\u001b[34m(self, weights, task)\u001b[39m\n\u001b[32m    292\u001b[39m weights = checks.check_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(weights).rpartition(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.ckpt = \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m.task = \u001b[38;5;28mself\u001b[39m.model.task\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m.overrides = \u001b[38;5;28mself\u001b[39m.model.args = \u001b[38;5;28mself\u001b[39m._reset_ckpt_args(\u001b[38;5;28mself\u001b[39m.model.args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ultralytics/nn/tasks.py:1548\u001b[39m, in \u001b[36mattempt_load_one_weight\u001b[39m\u001b[34m(weight, device, inplace, fuse)\u001b[39m\n\u001b[32m   1534\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mattempt_load_one_weight\u001b[39m(weight, device=\u001b[38;5;28;01mNone\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m, fuse=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1535\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1536\u001b[39m \u001b[33;03m    Load a single model weights.\u001b[39;00m\n\u001b[32m   1537\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1546\u001b[39m \u001b[33;03m        ckpt (dict): Model checkpoint dictionary.\u001b[39;00m\n\u001b[32m   1547\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1548\u001b[39m     ckpt, weight = \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[32m   1549\u001b[39m     args = {**DEFAULT_CFG_DICT, **(ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mtrain_args\u001b[39m\u001b[33m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[32m   1550\u001b[39m     model = (ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mema\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m]).to(device).float()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ultralytics/nn/tasks.py:1446\u001b[39m, in \u001b[36mtorch_safe_load\u001b[39m\u001b[34m(weight, safe_only)\u001b[39m\n\u001b[32m   1444\u001b[39m                 ckpt = torch.load(f, pickle_module=safe_pickle)\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1446\u001b[39m             ckpt = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.name == \u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ultralytics/utils/patches.py:119\u001b[39m, in \u001b[36mtorch_load\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweights_only\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    117\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mweights_only\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_torch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'checkpoints/yolo11_face_detection/model.pt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from edgeface.face_alignment import align\n",
    "from edgeface.backbones import get_model\n",
    "\n",
    "# load model\n",
    "model_name=\"edgeface_s_gamma_05\" # or edgeface_xs_gamma_06\n",
    "model=get_model(model_name)\n",
    "checkpoint_path=f'edgeface/checkpoints/{model_name}.pt'\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location='cpu')) # Load state dict\n",
    "model.eval() # Call eval() on the model object\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ])\n",
    "\n",
    "paths = 'edgeface/test/test_images/Elon_Musk.jpg'\n",
    "batch_size = len(paths) if isinstance(paths, (list, tuple)) else 1\n",
    "\n",
    "# Align faces (assuming align.get_aligned_face returns a list of tuples)\n",
    "aligned_result = align.get_aligned_face(paths, algorithm='yolo')\n",
    "\n",
    "transformed_inputs = [transform(result[1]) for result in aligned_result]\n",
    "transformed_inputs = torch.stack(transformed_inputs)\n",
    "\n",
    "# Extract embeddings\n",
    "embeddings = model(transformed_inputs)\n",
    "print(embeddings.shape)  # Expected: torch.Size([batch_size, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8345d1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/slim-face-recognition\n"
     ]
    }
   ],
   "source": [
    "%cd /workspaces/slim-face-recognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1de3a29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slim-face-recognition/\n",
      "├─.git/\n",
      "│ ├─info/\n",
      "│ │ ├─refs\n",
      "│ │ └─exclude\n",
      "│ ├─HEAD\n",
      "│ ├─packed-refs\n",
      "│ ├─logs/\n",
      "│ │ ├─HEAD\n",
      "│ │ └─refs/\n",
      "│ │   ├─remotes/\n",
      "│ │   │ └─origin/\n",
      "│ │   │   ├─dev\n",
      "│ │   │   └─main\n",
      "│ │   └─heads/\n",
      "│ │     └─dev\n",
      "│ ├─index\n",
      "│ ├─FETCH_HEAD\n",
      "│ ├─ORIG_HEAD\n",
      "│ ├─refs/\n",
      "│ │ ├─remotes/\n",
      "│ │ │ └─origin/\n",
      "│ │ │   ├─dev\n",
      "│ │ │   └─main\n",
      "│ │ ├─tags/\n",
      "│ │ └─heads/\n",
      "│ │   └─dev\n",
      "│ ├─COMMIT_EDITMSG\n",
      "│ └─branches/\n",
      "├─accelerate_train.py\n",
      "├─ztest.ipynb\n",
      "├─new_contruct.md\n",
      "├─requirements.txt\n",
      "├─checkpoints/\n",
      "│ ├─yolo11_face_detection/\n",
      "│ │ ├─.git/\n",
      "│ │ │ ├─info/\n",
      "│ │ │ │ └─exclude\n",
      "│ │ │ ├─HEAD\n",
      "│ │ │ ├─packed-refs\n",
      "│ │ │ ├─logs/\n",
      "│ │ │ │ ├─HEAD\n",
      "│ │ │ │ └─refs/\n",
      "│ │ │ │   ├─remotes/\n",
      "│ │ │ │   │ └─origin/\n",
      "│ │ │ │   │   └─HEAD\n",
      "│ │ │ │   └─heads/\n",
      "│ │ │ │     └─main\n",
      "│ │ │ ├─index\n",
      "│ │ │ ├─refs/\n",
      "│ │ │ │ ├─remotes/\n",
      "│ │ │ │ │ └─origin/\n",
      "│ │ │ │ │   └─HEAD\n",
      "│ │ │ │ ├─tags/\n",
      "│ │ │ │ └─heads/\n",
      "│ │ │ │   └─main\n",
      "│ │ │ ├─config\n",
      "│ │ │ ├─description\n",
      "│ │ │ ├─lfs/\n",
      "│ │ │ │ ├─incomplete/\n",
      "│ │ │ │ ├─tmp/\n",
      "│ │ │ │ └─objects/\n",
      "│ │ │ │   ├─2d/\n",
      "│ │ │ │   │ └─fe/\n",
      "│ │ │ │   │   └─2dfe14171f5b76a05f9bcf0dac7f94b7bff4416b1f29eff7c9ef5830f51c5719\n",
      "│ │ │ │   └─f5/\n",
      "│ │ │ │     └─b3/\n",
      "│ │ │ │       └─f5b3aaa834fd0acde177a0e84577458baa079d1ae5e9f500a9d03ca7d3a54c81\n",
      "│ │ │ └─hooks/\n",
      "│ │ │   ├─pre-commit.sample\n",
      "│ │ │   ├─update.sample\n",
      "│ │ │   ├─prepare-commit-msg.sample\n",
      "│ │ │   ├─pre-push.sample\n",
      "│ │ │   ├─fsmonitor-watchman.sample\n",
      "│ │ │   ├─push-to-checkout.sample\n",
      "│ │ │   ├─pre-push\n",
      "│ │ │   ├─post-commit\n",
      "│ │ │   ├─applypatch-msg.sample\n",
      "│ │ │   └─pre-receive.sample\n",
      "│ │ ├─result.png\n",
      "│ │ ├─model.pt\n",
      "│ │ ├─.gitattributes\n",
      "│ │ ├─Recall-confidence.png\n",
      "│ │ ├─model.onnx\n",
      "│ │ ├─README.md\n",
      "│ │ ├─config.json\n",
      "│ │ ├─Precision-confidence.png\n",
      "│ │ └─confusion-matrix.png\n",
      "│ └─face_classifier-epoch=00-val_loss=3.31.ckpt\n",
      "├─src/\n",
      "│ └─slim_face/\n",
      "│   ├─utils/\n",
      "│   │ ├─helpers.py\n",
      "│   │ ├─__pycache__/\n",
      "│   │ │ └─__init__.cpython-312.pyc\n",
      "│   │ └─__init__.py\n",
      "│   ├─training/\n",
      "│   │ ├─accelerate_train.py\n",
      "│   │ ├─train.py\n",
      "│   │ ├─accelerate_train_v2.py\n",
      "│   │ └─__init__.py\n",
      "│   ├─__init__.py\n",
      "│   └─models/\n",
      "│     ├─__pycache__/\n",
      "│     │ └─__init__.cpython-312.pyc\n",
      "│     ├─edgeface/\n",
      "│     │ ├─.git/\n",
      "│     │ │ ├─info/\n",
      "│     │ │ │ └─exclude\n",
      "│     │ │ ├─HEAD\n",
      "│     │ │ ├─packed-refs\n",
      "│     │ │ ├─logs/\n",
      "│     │ │ │ ├─HEAD\n",
      "│     │ │ │ └─refs/\n",
      "│     │ │ │   ├─remotes/\n",
      "│     │ │ │   │ └─origin/\n",
      "│     │ │ │   │   └─HEAD\n",
      "│     │ │ │   └─heads/\n",
      "│     │ │ │     └─main\n",
      "│     │ │ ├─index\n",
      "│     │ │ ├─refs/\n",
      "│     │ │ │ ├─remotes/\n",
      "│     │ │ │ │ └─origin/\n",
      "│     │ │ │ │   └─HEAD\n",
      "│     │ │ │ ├─tags/\n",
      "│     │ │ │ └─heads/\n",
      "│     │ │ │   └─main\n",
      "│     │ │ ├─config\n",
      "│     │ │ ├─description\n",
      "│     │ │ ├─hooks/\n",
      "│     │ │ │ ├─pre-commit.sample\n",
      "│     │ │ │ ├─update.sample\n",
      "│     │ │ │ ├─prepare-commit-msg.sample\n",
      "│     │ │ │ ├─pre-push.sample\n",
      "│     │ │ │ ├─fsmonitor-watchman.sample\n",
      "│     │ │ │ ├─push-to-checkout.sample\n",
      "│     │ │ │ ├─applypatch-msg.sample\n",
      "│     │ │ │ ├─pre-receive.sample\n",
      "│     │ │ │ ├─post-update.sample\n",
      "│     │ │ │ └─sendemail-validate.sample\n",
      "│     │ │ └─objects/\n",
      "│     │ │   ├─info/\n",
      "│     │ │   └─pack/\n",
      "│     │ │     ├─pack-20bfa1e7d228c9f1a1ab58e164316d45d6573f2b.pack\n",
      "│     │ │     ├─pack-20bfa1e7d228c9f1a1ab58e164316d45d6573f2b.idx\n",
      "│     │ │     └─pack-20bfa1e7d228c9f1a1ab58e164316d45d6573f2b.rev\n",
      "│     │ ├─test/\n",
      "│     │ │ ├─test_yolo_face_detection.py\n",
      "│     │ │ ├─test_face_detection_mtcnn.py\n",
      "│     │ │ ├─test_images/\n",
      "│     │ │ │ ├─006_9135205d.jpg\n",
      "│     │ │ │ ├─006_87166f38.jpg\n",
      "│     │ │ │ ├─002_6749a2c4.jpg\n",
      "│     │ │ │ ├─Elon_Musk.jpg\n",
      "│     │ │ │ ├─028_6a0ff8de.jpg\n",
      "│     │ │ │ ├─Meryl Streep.jpg\n",
      "│     │ │ │ ├─Robert De Niro.jpg\n",
      "│     │ │ │ ├─Daniel Day-Lewis.jpg\n",
      "│     │ │ │ ├─005_9574c208.jpg\n",
      "│     │ │ │ └─025_c55c4e11.jpg\n",
      "│     │ │ ├─__init__.py\n",
      "│     │ │ └─test_yolo_face_detection_with_batch.py\n",
      "│     │ ├─requirements.txt\n",
      "│     │ ├─utils/\n",
      "│     │ │ ├─download_yolo_face_detection.py\n",
      "│     │ │ ├─__pycache__/\n",
      "│     │ │ │ ├─__init__.cpython-312.pyc\n",
      "│     │ │ │ └─download_yolo_face_detection.cpython-312.pyc\n",
      "│     │ │ └─__init__.py\n",
      "│     │ ├─checkpoints/\n",
      "│     │ │ ├─edgeface_xs_gamma_06.pt\n",
      "│     │ │ ├─yolo11_face_detection/\n",
      "│     │ │ │ ├─.git/\n",
      "│     │ │ │ │ ├─info/\n",
      "│     │ │ │ │ │ └─exclude\n",
      "│     │ │ │ │ ├─HEAD\n",
      "│     │ │ │ │ ├─packed-refs\n",
      "│     │ │ │ │ ├─logs/\n",
      "│     │ │ │ │ │ ├─HEAD\n",
      "│     │ │ │ │ │ └─refs/\n",
      "│     │ │ │ │ │   ├─remotes/\n",
      "│     │ │ │ │ │   │ └─origin/\n",
      "│     │ │ │ │ │   │   └─HEAD\n",
      "│     │ │ │ │ │   └─heads/\n",
      "│     │ │ │ │ │     └─main\n",
      "│     │ │ │ │ ├─index\n",
      "│     │ │ │ │ ├─refs/\n",
      "│     │ │ │ │ │ ├─remotes/\n",
      "│     │ │ │ │ │ │ └─origin/\n",
      "│     │ │ │ │ │ │   └─HEAD\n",
      "│     │ │ │ │ │ ├─tags/\n",
      "│     │ │ │ │ │ └─heads/\n",
      "│     │ │ │ │ │   └─main\n",
      "│     │ │ │ │ ├─config\n",
      "│     │ │ │ │ ├─description\n",
      "│     │ │ │ │ ├─lfs/\n",
      "│     │ │ │ │ │ ├─incomplete/\n",
      "│     │ │ │ │ │ ├─tmp/\n",
      "│     │ │ │ │ │ └─objects/\n",
      "│     │ │ │ │ │   ├─2d/\n",
      "│     │ │ │ │ │   │ └─fe/\n",
      "│     │ │ │ │ │   │   └─2dfe14171f5b76a05f9bcf0dac7f94b7bff4416b1f29eff7c9ef5830f51c5719\n",
      "│     │ │ │ │ │   └─f5/\n",
      "│     │ │ │ │ │     └─b3/\n",
      "│     │ │ │ │ │       └─f5b3aaa834fd0acde177a0e84577458baa079d1ae5e9f500a9d03ca7d3a54c81\n",
      "│     │ │ │ │ └─hooks/\n",
      "│     │ │ │ │   ├─pre-commit.sample\n",
      "│     │ │ │ │   ├─update.sample\n",
      "│     │ │ │ │   ├─prepare-commit-msg.sample\n",
      "│     │ │ │ │   ├─pre-push.sample\n",
      "│     │ │ │ │   ├─fsmonitor-watchman.sample\n",
      "│     │ │ │ │   ├─push-to-checkout.sample\n",
      "│     │ │ │ │   ├─pre-push\n",
      "│     │ │ │ │   ├─post-commit\n",
      "│     │ │ │ │   ├─applypatch-msg.sample\n",
      "│     │ │ │ │   └─pre-receive.sample\n",
      "│     │ │ │ ├─result.png\n",
      "│     │ │ │ ├─model.pt\n",
      "│     │ │ │ ├─.gitattributes\n",
      "│     │ │ │ ├─Recall-confidence.png\n",
      "│     │ │ │ ├─model.onnx\n",
      "│     │ │ │ ├─README.md\n",
      "│     │ │ │ ├─config.json\n",
      "│     │ │ │ ├─Precision-confidence.png\n",
      "│     │ │ │ └─confusion-matrix.png\n",
      "│     │ │ └─edgeface_s_gamma_05.pt\n",
      "│     │ ├─.python-version\n",
      "│     │ ├─face_alignment/\n",
      "│     │ │ ├─align.py\n",
      "│     │ │ ├─face_yolo.py\n",
      "│     │ │ ├─README.md\n",
      "│     │ │ ├─LICENSE\n",
      "│     │ │ ├─__pycache__/\n",
      "│     │ │ │ ├─face_yolo.cpython-312.pyc\n",
      "│     │ │ │ ├─align.cpython-312.pyc\n",
      "│     │ │ │ └─mtcnn.cpython-312.pyc\n",
      "│     │ │ ├─mtcnn.py\n",
      "│     │ │ └─mtcnn_pytorch/\n",
      "│     │ │   ├─extract_weights_from_caffe_models.py\n",
      "│     │ │   ├─try_mtcnn_step_by_step.ipynb\n",
      "│     │ │   ├─src/\n",
      "│     │ │   │ ├─matlab_cp2tform.py\n",
      "│     │ │   │ ├─get_nets.py\n",
      "│     │ │   │ ├─detector.py\n",
      "│     │ │   │ ├─visualization_utils.py\n",
      "│     │ │   │ ├─align_trans.py\n",
      "│     │ │   │ ├─__pycache__/\n",
      "│     │ │   │ │ ├─align_trans.cpython-312.pyc\n",
      "│     │ │   │ │ ├─__init__.cpython-312.pyc\n",
      "│     │ │   │ │ ├─first_stage.cpython-312.pyc\n",
      "│     │ │   │ │ ├─visualization_utils.cpython-312.pyc\n",
      "│     │ │   │ │ ├─box_utils.cpython-312.pyc\n",
      "│     │ │   │ │ ├─matlab_cp2tform.cpython-312.pyc\n",
      "│     │ │   │ │ ├─detector.cpython-312.pyc\n",
      "│     │ │   │ │ └─get_nets.cpython-312.pyc\n",
      "│     │ │   │ ├─weights/\n",
      "│     │ │   │ │ ├─rnet.npy\n",
      "│     │ │   │ │ ├─onet.npy\n",
      "│     │ │   │ │ └─pnet.npy\n",
      "│     │ │   │ ├─__init__.py\n",
      "│     │ │   │ ├─first_stage.py\n",
      "│     │ │   │ └─box_utils.py\n",
      "│     │ │   ├─test_on_images.ipynb\n",
      "│     │ │   ├─get_aligned_face_from_mtcnn.ipynb\n",
      "│     │ │   ├─README.md\n",
      "│     │ │   ├─LICENSE\n",
      "│     │ │   ├─refine_faces.ipynb\n",
      "│     │ │   ├─caffe_models/\n",
      "│     │ │   │ ├─det3.prototxt\n",
      "│     │ │   │ ├─det2.caffemodel\n",
      "│     │ │   │ ├─det2.prototxt\n",
      "│     │ │   │ ├─det1.prototxt\n",
      "│     │ │   │ ├─det1.caffemodel\n",
      "│     │ │   │ ├─det4.caffemodel\n",
      "│     │ │   │ ├─det4.prototxt\n",
      "│     │ │   │ └─det3.caffemodel\n",
      "│     │ │   └─.gitignore\n",
      "│     │ ├─hubconf.py\n",
      "│     │ ├─README.md\n",
      "│     │ └─LICENSE\n",
      "│     └─__init__.py\n",
      "├─.python-version\n",
      "├─tests/\n",
      "│ ├─README.md\n",
      "│ ├─test_images/\n",
      "│ │ └─Elon_Musk.jpg\n",
      "│ ├─__init__.py\n",
      "│ └─export_image_embeding.py\n",
      "└─ckpts/\n",
      "  ├─edgeface_ckpts/\n",
      "  │ ├─edgeface_xs_gamma_06.pt\n",
      "  │ └─edgeface_s_gamma_05.pt\n",
      "  └─README.md\n"
     ]
    }
   ],
   "source": [
    "import seedir as sd\n",
    "\n",
    "# Display directory structure with a limit of 10 items\n",
    "sd.seedir(path='.', itemlimit=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
